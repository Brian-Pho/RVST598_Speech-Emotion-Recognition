\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage{graphicx}
\graphicspath{{figures/}}

% Copyright
%\setcopyright{none}
\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\acmDOI{xx.xxx/xxx_x}

% ISBN
\acmISBN{978-1-4503-6866-7/20/03}

%Conference
\acmConference[SAC'20]{ACM SAC Conference}{March 30-April 3, 2020}{Brno, Czech Republic} 
\acmYear{2020}
\copyrightyear{2020}


\acmArticle{4}
\acmPrice{15.00}

% These commands are optional
%\acmBooktitle{Transactions of the ACM Woodstock conference}
%\editor{Jennifer B. Sartor}
%\editor{Theo D'Hondt}
%\editor{Wolfgang De Meuter}


\begin{document}
\title{Multi-Label Speech Emotion Recognition Using 2D Convolutional Neural Networks}
\subtitle{Using machine learning to identify multiple emotions in speech}


\author{Brian Pho}
\affiliation{%
	\institution{University of Calgary}
	\streetaddress{2500 University Drive NW}
	\city{Calgary} 
	\state{Alberta} 
	\postcode{T2N 1N4}
}
\email{brian.pho@ucalgary.ca}

\author{Thomas Truong}
\affiliation{%
	\institution{University of Calgary}
	\streetaddress{2500 University Drive NW}
	\city{Calgary} 
	\state{Alberta} 
	\postcode{T2N 1N4}
}
\email{thomas.truong@ucalgary.ca}

\author{Svetlana Yanushkevich}
\affiliation{%
	\institution{University of Calgary}
	\streetaddress{2500 University Drive NW}
	\city{Calgary} 
	\state{Alberta} 
	\postcode{T2N 1N4}
}
\email{syanshk@ucalgary.ca}

% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{B. Pho et al.}


\begin{abstract}
A problem with current speech emotion recognition systems is that they can only recognize one emotion from a set of emotions in speech. This is a problem because it does not match how people recognize emotions in speech. When people hear speech, they can perceive multiple emotions instead of a single emotion. We address this problem by building a 2D convolutional neural network that can also recognize multiple emotions and it achieves an accuracy of 57.64\%. This model demonstrates a more realistic approach to speech emotion recognition by more closely matching human data.\footnote{Code for this paper is available on Github at: \url{https://github.com/Brian-Pho/RVST598_Speech-Emotion-Recognition}}
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
\begin{CCSXML}
	<ccs2012>
	<concept>
	<concept_id>10010405.10010469.10010475</concept_id>
	<concept_desc>Applied computing~Sound and music computing</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Applied computing~Sound and music computing}


\keywords{multi-label, speech, emotion, recognition, neural networks}


\maketitle

\input{paper_body}

\bibliographystyle{ACM-Reference-Format}
\bibliography{references} 

\end{document}
