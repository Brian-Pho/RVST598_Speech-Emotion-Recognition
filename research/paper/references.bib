% Encoding: UTF-8

@article{busso_2008,
	title={IEMOCAP: interactive emotional dyadic motion capture database},
	volume={42},
	DOI={10.1007/s10579-008-9076-6},
	number={4},
	journal={Language Resources and Evaluation},
	author={Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N. and Lee, Sungbok and Narayanan, Shrikanth S.},
	year={2008},
	pages={335-359}
},

@article{cao_2014,
	title={CREMA-D: Crowd-Sourced Emotional Multimodal Actors Dataset},
	volume={5},
	DOI={10.1109/taffc.2014.2336244},
	number={4},
	journal={IEEE Transactions on Affective Computing},
	author={Cao, Houwei and Cooper, David G. and Keutmann, Michael K. and Gur, Ruben C. and Nenkova, Ani and Verma, Ragini},
	year={2014},
	pages={377-390}
},

@article{livingstone_2018,
	title={The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English},
	volume={13},
	DOI={10.1371/journal.pone.0196391},
	number={5},
	journal={PLOS ONE},
	author={Livingstone, Steven R. and Russo, Frank A.},
	year={2018},
	pages={e0196391}
}

@article{dupuis_2011, 
	title={Recognition of emotional speech for younger and older talkers: Behavioural findings from the toronto emotional speech set}, 
	volume={39}, 
	url={https://jcaa.caa-aca.ca/index.php/jcaa/article/view/2471}, 
	abstractNote={A study that was conducted to analyze recognition of emotional speech for younger and older talkers is presented. Each actor recorded the stimuli individually in a sound- attenuating booth for approximately 20 hours. During the recording sessions, which typically lasted three to four hours, the majority of the time was spent creating the voice recordings, while approximately 10% of the time was devoted to practicing and fine-tuning each actor’s portrayal of each of the emotions. Three female undergraduate students with normal hearing listened to the stimuli and identified, for each actor, which token of each NU-6 item they considered to be the most representative for each of the seven emotions. The experimenter used the same procedure to listen to each of the sound files.}, 
	number={3}, 
	journal={Canadian Acoustics}, 
	author={Dupuis, Kate and Kathleen Pichora-Fuller, M.}, 
	year={2011}, 
	month={Sep.}, 
	pages={182-183} 
}

@inproceedings{Yenigalla2018,
	abstract = {This paper presents a speech emotion recognition system using a recurrent neural network (RNN) model trained by an efficient learning algorithm. The proposed system takes into account the long-range context effect and the uncertainty of emotional label expressions. To extract high-level representation of emotional states with regard to its temporal dynamics, a powerful learning method with a bidirectional long short-term memory (BLSTM) model is adopted. To overcome the uncertainty of emotional la-bels, such that all frames in the same utterance are mapped into the same emotional label, it is assumed that the label of each frame is regarded as a sequence of random variables. Then, the sequences are trained by the proposed learning algorithm. The weighted accuracy of the proposed emotion recognition system is improved up to 12{\%} compared to the DNN-ELM based emo-tion recognition system used as a baseline.},
	author = {Yenigalla, Promod and Kumar, Abhay and Tripathi, Suraj and Singh, Chirag and Kar, Sibsambhu and Vepa, Jithendra},
	booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
	doi = {10.21437/Interspeech.2018-1811},
	file = {:C$\backslash$:/Users/Thomas/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yenigalla et al. - 2018 - Speech emotion recognition using spectrogram {\&} phoneme embedding.pdf:pdf},
	issn = {19909772},
	keywords = {CNN,Phoneme,Phoneme embedding,Spectrogram,Speech emotion recognition},
	pages = {3688--3692},
	publisher = {International Speech Communication Association},
	title = {{Speech emotion recognition using spectrogram {\&} phoneme embedding}},
	volume = {2018-September},
	year = {2018}
}
@article{Rana2016,
	abstract = {This paper investigates the performance of Deep Learning for speech emotion classification when the speech is compounded with noise. It reports on the classification accuracy and concludes with the future directions for achieving greater robustness for emotion recognition from noisy speech.},
	archivePrefix = {arXiv},
	arxivId = {1603.05901},
	author = {Rana, Rajib},
	eprint = {1603.05901},
	file = {::},
	month = {mar},
	title = {{Emotion Classification from Noisy Speech - A Deep Learning Approach}},
	url = {http://arxiv.org/abs/1603.05901},
	year = {2016}
}
@inproceedings{Zhang2019,
	abstract = {Speech emotion recognition is a challenging task for three main reasons: 1) human emotion is abstract, which means it is hard to distinguish; 2) in general, human emotion can only be detected in some specific moments during a long utterance; 3) speech data with emotional labeling is usually limited. In this paper, we present a novel attention based fully convolutional network for speech emotion recognition. We employ fully convolutional network as it is able to handle variable-length speech, free of the demand of segmentation to keep critical information not lost. The proposed attention mechanism can make our model be aware of which time-frequency region of speech spectrogram is more emotion-relevant. Considering limited data, the transfer learning is also adapted to improve the accuracy. Especially, it's interesting to observe obvious improvement obtained with natural scene image based pre-trained model. Validated on the publicly available IEMOCAP corpus, the proposed model outperformed the state-of-the-art methods with a weighted accuracy of 70.4{\%} and an unweighted accuracy of 63.9{\%} respectively.},
	author = {Zhang, Yuanyuan and Du, Jun and Wang, Zirui and Zhang, Jianshu and Tu, Yanhui},
	booktitle = {2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2018 - Proceedings},
	doi = {10.23919/APSIPA.2018.8659587},
	file = {::},
	isbn = {9789881476852},
	month = {mar},
	pages = {1771--1775},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	title = {{Attention Based Fully Convolutional Network for Speech Emotion Recognition}},
	year = {2019}
}
@article{Engel2019,
	abstract = {Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence. Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms. Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.},
	archivePrefix = {arXiv},
	arxivId = {1902.08710},
	author = {Engel, Jesse and Agrawal, Kumar Krishna and Chen, Shuo and Gulrajani, Ishaan and Donahue, Chris and Roberts, Adam},
	eprint = {1902.08710},
	file = {::},
	month = {feb},
	title = {{GANSynth: Adversarial Neural Audio Synthesis}},
	url = {http://arxiv.org/abs/1902.08710},
	year = {2019}
}
@misc{Koolagudi2012,
	abstract = {In recent years the research interest is improving in the field of human computer interaction. This paper focus on one of the aspect of human computer interaction in concern with, the recognition of emotion in a person with the help of Electroencephalogram (EEG) signals and speech. EEG uses an electrical activity of the neurons inside the brain. EEG machine is used for acquisition of the electrical potential generated by the neurons when they are active. The Brain cells communicate with each other by sending electrical impulses. Emotions allow people to express themselves beyond the verbal domain. Speech is the most natural form of communication. A much of work is been done in speech recognition in various languages. It is one of the components that closely related to emotions. Very less work has been carried out using combine aspects of speech, emotion and EEG. Thus this paper attempts to review the combine efforts of EEG brain signal and Speech to recognize the emotions in humans.},
	author = {Koolagudi, Shashidhar G. and Rao, K. Sreenivasa},
	booktitle = {International Journal of Speech Technology},
	doi = {10.1007/s10772-011-9125-1},
	file = {::},
	issn = {13812416},
	keywords = {Classification models,Elicited speech corpus,Emotion recognition,Excitation source features,Natural speech corpus,Prosodic features,Simulated emotional speech corpus,System features},
	month = {jun},
	number = {2},
	pages = {99--117},
	title = {{Emotion recognition from speech: A review}},
	volume = {15},
	year = {2012}
}
@incollection{Kerkeni2019,
	abstract = {Abstract Long-haul travel does not constitute an obstacle for tourists to travel and is fast gaining the attention of tourists in new and unique experiences. This study was conducted to identify the long-haul travel motivation by international tourists to Penang. A total of 400 respondents participated in this survey, conducted around the tourist attractions in Penang, using cluster random sampling. However, only 370 questionnaires were only used for this research. Data were analysed using SPSS software 22 version. The findings, ‘knowledge and novelty seeking' were the main push factors that drove long-haul travel by international tourists to Penang. Meanwhile, the main pull factor that attracts long- haul travel by international tourists to Penang was its ‘culture and history'. Additionally, there were partly direct and significant relationships between socio-demographic, trip characteristics and travel motivation (push factors and pull factors). Overall, this study identified the long-haul travel motivations by international tourists to Penang based on socio-demographic, trip characteristics and travel motivation and has indirectly helped in understanding the long-haul travel market particularly for Penang and Southeast Asia. This research also suggested for an effective marketing and promotion strategy in pro- viding useful information that is the key to attract international tourists to travel long distances. Keywords:},
	author = {Kerkeni, Leila and Serrestou, Youssef and Mbarki, Mohamed and Raoof, Kosai and {Ali Mahjoub}, Mohamed and Cleder, Catherine},
	booktitle = {Social Media and Machine Learning [Working Title]},
	doi = {10.5772/intechopen.84856},
	file = {::},
	month = {mar},
	publisher = {IntechOpen},
	title = {{Automatic Speech Emotion Recognition Using Machine Learning}},
	url = {https://www.intechopen.com/online-first/automatic-speech-emotion-recognition-using-machine-learning},
	year = {2019}
}
@techreport{Balakrishnan2017,
	abstract = {Emotions are integral to communication. Over the past few years, we have aimed to create technology to convey emotions through emojis and speech generation. The paper describes an experimental study on the detection of emotion from speech. This study utilizes a corpus containing continuous emotional speech comprised of 9 different emotions: Activation, Valence, Power, Intensity, Amusement, Happiness, Sadness, and Anger. We created an RNN-based model and a CNN-based model that can detect emotions in speech with reason-able performance against a random base-line and a simple neural network.},
	author = {Balakrishnan, Anusha and Rege, Alisha},
	file = {::},
	keywords = {anusha,balakirshnan},
	title = {{Reading Emotions from Speech using Deep Neural Networks}},
	year = {2017}
}
@article{Selvaraj2016,
	abstract = {In human machine interface application, emotion recognition from the speech signal has been research topic since many years. To identify the emotions from the speech signal, many systems have been developed. In this paper speech emotion recognition based on the previous technologies which uses different classifiers for the emotion recognition is reviewed. The classifiers are used to differentiate emotions such as anger, happiness, sadness, surprise, neutral state, etc. The database for the speech emotion recognition system is the emotional speech samples and the features extracted from these speech samples are the energy, pitch, linear prediction cepstrum coefficient (LPCC), Mel frequency cepstrum coefficient (MFCC). The classification performance is based on extracted features. Inference about the performance and limitation of speech emotion recognition system based on the different classifiers are also discussed.},
	author = {Selvaraj, Maheshwari and Bhuvana, R. and Padmaja, S.},
	file = {::},
	issn = {09754024},
	journal = {International Journal of Engineering and Technology},
	keywords = {Back propagation network,MFCC,Prosodic features,Radial basis function network,Speech emotion recognition,Support vector machine},
	month = {feb},
	number = {1},
	pages = {311--323},
	publisher = {Engg Journals Publications},
	title = {{Human speech emotion recognition}},
	volume = {8},
	year = {2016}
}
@inproceedings{Wang2017,
	abstract = {Accurately recognizing speaker emotion and age/gender from speech can provide better user experience for many spoken dialogue systems. In this study, we propose to use deep neural networks (DNNs) to encode each utterance into a fixed-length vector by pooling the activations of the last hidden layer over time. The feature encoding process is designed to be jointly trained with the utterance-level classifier for better classification. A kernel extreme learning machine (ELM) is further trained on the encoded vectors for better utterance-level classification. Experiments on a Mandarin dataset demonstrate the effectiveness of our proposed methods on speech emotion and age/gender recognition tasks.},
	author = {Wang, Zhong Qiu and Tashev, Ivan},
	booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
	doi = {10.1109/ICASSP.2017.7953138},
	file = {::},
	isbn = {9781509041176},
	issn = {15206149},
	keywords = {deep neural networks,kernel extreme learning machine,pooling,speech age/gender recognition,speech emotion recognition},
	month = {jun},
	pages = {5150--5154},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	title = {{Learning utterance-level representations for speech emotion and age/gender recognition using deep neural networks}},
	year = {2017}
}
@article{Zhao2019,
	abstract = {We aimed at learning deep emotion features to recognize speech emotion. Two convolutional neural network and long short-term memory (CNN LSTM) networks, one 1D CNN LSTM network and one 2D CNN LSTM network, were constructed to learn local and global emotion-related features from speech and log-mel spectrogram respectively. The two networks have the similar architecture, both consisting of four local feature learning blocks (LFLBs) and one long short-term memory (LSTM) layer. LFLB, which mainly contains one convolutional layer and one max-pooling layer, is built for learning local correlations along with extracting hierarchical correlations. LSTM layer is adopted to learn long-term dependencies from the learned local features. The designed networks, combinations of the convolutional neural network (CNN) and LSTM, can take advantage of the strengths of both networks and overcome the shortcomings of them, and are evaluated on two benchmark databases. The experimental results show that the designed networks achieve excellent performance on the task of recognizing speech emotion, especially the 2D CNN LSTM network outperforms the traditional approaches, Deep Belief Network (DBN) and CNN on the selected databases. The 2D CNN LSTM network achieves recognition accuracies of 95.33{\%} and 95.89{\%} on Berlin EmoDB of speaker-dependent and speaker-independent experiments respectively, which compare favourably to the accuracy of 91.6{\%} and 92.9{\%} obtained by traditional approaches; and also yields recognition accuracies of 89.16{\%} and 52.14{\%} on IEMOCAP database of speaker-dependent and speaker-independent experiments, which are much higher than the accuracy of 73.78{\%} and 40.02{\%} obtained by DBN and CNN.},
	author = {Zhao, Jianfeng and Mao, Xia and Chen, Lijiang},
	doi = {10.1016/j.bspc.2018.08.035},
	file = {::},
	issn = {17468108},
	journal = {Biomedical Signal Processing and Control},
	keywords = {CNN LSTM network,Log-mel spectrograms,Raw audio clips,Speech emotion recognition},
	month = {jan},
	pages = {312--323},
	publisher = {Elsevier},
	title = {{Speech emotion recognition using deep 1D {\&} 2D CNN LSTM networks}},
	url = {https://www.sciencedirect.com/science/article/pii/S1746809418302337{\#}bib0265},
	volume = {47},
	year = {2019}
}
@article{Ozseven2019,
	abstract = {Speech emotion recognition involves analyzing vocal changes caused by emotions with acoustic analysis and determining the features to be used for emotion recognition. The number of features obtained by acoustic analysis reaches very high values depending on the number of acoustic parameters used and statistical variations of these parameters. Not all of these features are effective for emotion recognition; in addition, different emotions may effect different vocal features. For this reason, feature selection methods are used to increase the emotional recognition success and reduce workload with fewer features. There is no certainty that existing feature selection methods increase/decrease emotion recognition success; some of these methods increase the total workload. In this study, a new statistical feature selection method is proposed based on the changes in emotions on acoustic features. The success of the proposed method is compared with other methods mostly used in literature. The comparison was made based on number of feature and emotion recognition success. According to the results obtained, the proposed method provides a significant reduction in the number of features, as well as increasing the classification success.},
	author = {{\"{O}}zseven, Turgut},
	doi = {10.1016/j.apacoust.2018.11.028},
	file = {::},
	issn = {1872910X},
	journal = {Applied Acoustics},
	keywords = {Emotion recognition,Feature selection,Speech emotion recognition,Speech processing},
	month = {mar},
	pages = {320--326},
	publisher = {Elsevier},
	title = {{A novel feature selection method for speech emotion recognition}},
	url = {https://www.sciencedirect.com/science/article/pii/S0003682X18309915},
	volume = {146},
	year = {2019}
}
@article{Hossain2019,
	abstract = {This paper proposes an emotion recognition system using a deep learning approach from emotional Big Data. The Big Data comprises of speech and video. In the proposed system, a speech signal is first processed in the frequency domain to obtain a Mel-spectrogram, which can be treated as an image. Then this Mel-spectrogram is fed to a convolutional neural network (CNN). For video signals, some representative frames from a video segment are extracted and fed to the CNN. The outputs of the two CNNs are fused using two consecutive extreme learning machines (ELMs). The output of the fusion is given to a support vector machine (SVM) for final classification of the emotions. The proposed system is evaluated using two audio–visual emotional databases, one of which is Big Data. Experimental results confirm the effectiveness of the proposed system involving the CNNs and the ELMs.},
	author = {Hossain, M. Shamim and Muhammad, Ghulam},
	doi = {10.1016/j.inffus.2018.09.008},
	file = {::},
	issn = {15662535},
	journal = {Information Fusion},
	month = {sep},
	pages = {69--78},
	publisher = {Elsevier},
	title = {{Emotion recognition using deep learning approach from audio–visual emotional big data}},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253517307066},
	volume = {49},
	year = {2019}
}
@article{Badshah2019,
	abstract = {{\textcopyright} 2017 Springer Science+Business Media, LLC Emotion recognition from speech signals is an interesting research with several applications like smart healthcare, autonomous voice response systems, assessing situational seriousness by caller affective state analysis in emergency centers, and other smart affective services. In this paper, we present a study of speech emotion recognition based on the features extracted from spectrograms using a deep convolutional neural network (CNN) with rectangular kernels. Typically, CNNs have square shaped kernels and pooling operators at various layers, which are suited for 2D image data. However, in case of spectrograms, the information is encoded in a slightly different manner. Time is represented along the x-axis and y-axis shows frequency of the speech signal, whereas, the amplitude is indicated by the intensity value in the spectrogram at a particular position. To analyze speech through spectrograms, we propose rectangular kernels of varying shapes and sizes, along with max pooling in rectangular neighborhoods, to extract discriminative features. The proposed scheme effectively learns discriminative features from speech spectrograms and performs better than many state-of-the-art techniques when evaluated its performance on Emo-DB and Korean speech dataset.},
	author = {Badshah, Abdul Malik and Rahim, Nasir and Ullah, Noor and Ahmad, Jamil and Muhammad, Khan and Lee, Mi Young and Kwon, Soonil and Baik, Sung Wook},
	doi = {10.1007/s11042-017-5292-7},
	file = {::},
	issn = {15737721},
	journal = {Multimedia Tools and Applications},
	keywords = {Convolutional neural network,Rectangular kernels,Spectrogram,Speech emotion recognition},
	month = {mar},
	number = {5},
	pages = {5571--5589},
	publisher = {Springer US},
	title = {{Deep features-based speech emotion recognition for smart affective services}},
	url = {http://link.springer.com/10.1007/s11042-017-5292-7},
	volume = {78},
	year = {2019}
}
@article{Albanie2018,
	abstract = {Obtaining large, human labelled speech datasets to train models for emotion recognition is a notoriously challenging task, hindered by annotation cost and label ambiguity. In this work, we consider the task of learning embeddings for speech classification without access to any form of labelled audio. We base our approach on a simple hypothesis: that the emotional content of speech correlates with the facial expression of the speaker. By exploiting this relationship, we show that annotations of expression can be transferred from the visual domain (faces) to the speech domain (voices) through cross-modal distillation. We make the following contributions: (i) we develop a strong teacher network for facial emotion recognition that achieves the state of the art on a standard benchmark; (ii) we use the teacher to train a student, tabula rasa, to learn representations (embeddings) for speech emotion recognition without access to labelled audio data; and (iii) we show that the speech emotion embedding can be used for speech emotion recognition on external benchmark datasets. Code, models and data are available.},
	archivePrefix = {arXiv},
	arxivId = {1808.05561},
	author = {Albanie, Samuel and Nagrani, Arsha and Vedaldi, Andrea and Zisserman, Andrew},
	doi = {10.1145/3240508.3240578},
	eprint = {1808.05561},
	file = {::},
	isbn = {9781450356657},
	journal = {MM 2018 - Proceedings of the 2018 ACM Multimedia Conference},
	keywords = {Cross-modal transfer,Speech emotion recognition},
	month = {aug},
	pages = {292--301},
	title = {{Emotion recognition in speech using cross-modal transfer in the wild}},
	url = {http://arxiv.org/abs/1808.05561},
	year = {2018}
}
@inproceedings{Tzirakis2018,
	abstract = {Affect recognition is an important component towards the better interaction between human and machines. Applications of emotion recognition in speech can be found in several areas such as human computer interaction and call centres.},
	author = {Tzirakis, Panagiotis and Zhang, Jiehao and Schuller, Bjorn W.},
	booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
	doi = {10.1109/ICASSP.2018.8462677},
	file = {::},
	isbn = {9781538646588},
	issn = {15206149},
	keywords = {Deep learning,End-to-end learning,Speech emotion recognition},
	month = {apr},
	pages = {5089--5093},
	publisher = {IEEE},
	title = {{End-to-end speech emotion recognition using deep neural networks}},
	url = {https://ieeexplore.ieee.org/document/8462677/},
	volume = {2018-April},
	year = {2018}
}
@article{Etienne2018,
	abstract = {In this work we design a neural network for recognizing emotions in speech, using the IEMOCAP dataset. Following the latest advances in audio analysis, we use an architecture involving both convolutional layers, for extracting high-level features from raw spectrograms, and recurrent ones for aggregating long-term dependencies. We examine the techniques of data augmentation with vocal track length perturbation, layer-wise optimizer adjustment, batch normalization of recurrent layers and obtain highly competitive results of 64.5{\%} for weighted accuracy and 61.7{\%} for unweighted accuracy on four emotions.},
	archivePrefix = {arXiv},
	arxivId = {1802.05630},
	author = {Etienne, Caroline and Fidanza, Guillaume and Petrovskii, Andrei and Devillers, Laurence and Schmauch, Benoit},
	doi = {10.21437/smm.2018-5},
	eprint = {1802.05630},
	file = {::},
	month = {feb},
	pages = {21--25},
	title = {{CNN+LSTM Architecture for Speech Emotion Recognition with Data Augmentation}},
	url = {http://arxiv.org/abs/1802.05630 http://dx.doi.org/10.21437/SMM.2018-5},
	year = {2018}
}
@article{Chen2018,
	abstract = {Benzene is an occupational and environmental toxicant. The main human health concern associated with benzene exposure is acute myelogenous leukemia. Benzene produces lung tumors in mice, while its effects on human lung are not clear. The adverse effects of benzene are dependent on its metabolism by the cytochrome P-450 enzyme system. The isozymes CYP2E1 and CYP2F2 play roles in the metabolism of benzene at low, environmentally relevant concentrations. Previous studies indicate that the mouse lung readily metabolizes benzene and that CYP2F2 plays a role in this biotransformation. The significance of CYP2E1 and CYP2F2 in benzene metabolism was determined by measuring their apparent kinetic parameters K(m) and V(max). Use of wild-type and CYP2E1 knockout mice and selective inhibitors allowed the determination of the individual importance of both CYP2E1 and CYP2F2 in mouse liver and lung. A simple Michaelis-Menten relationship involving Lineweaver-Burk plots for the microsomal metabolism of benzene shows the apparent kinetic factors are different between the wild-type (K(m): 30.4 microM, V(max): 25.3 pmol/mg protein/min) and knockout (K(m): 1.9 microM, V(max): 0.5 pmol/mg protein/min) mouse livers. Wild-type lung has a K(m) of 2.3 microM and V(max) of 0.9 pmol/mg protein/min. CYP2E1 knockout lung has similar affinity and metabolic activity with a K(m) of 3.7 microM and V(max) of 1.2 pmol/mg protein/min. These data suggest CYP2E1 is less important in the lung than liver, and that it has a lower affinity for benzene but higher rate of hydroxylated metabolite production than does CYP2F2, which plays the predominant role in metabolizing benzene in mouse lung},
	author = {Chen, Mingyi and He, Xuanji and Yang, Jing and Zhang, Han},
	doi = {10.1109/LSP.2018.2860246},
	file = {::},
	issn = {10709908},
	journal = {IEEE Signal Processing Letters},
	keywords = {Attention mechanism,convolutional recurrent neural networks (CRNN),speech emotion recognition (SER)},
	month = {oct},
	number = {10},
	pages = {1440--1444},
	title = {{3-D Convolutional Recurrent Neural Networks with Attention Model for Speech Emotion Recognition}},
	url = {https://ieeexplore.ieee.org/document/8421023/},
	volume = {25},
	year = {2018}
}
@inproceedings{Zhao2018,
	abstract = {Traditional speech emotion recognition is based on the pipeline of pre-processing, feature extraction, dimensionality reduction and classification. Recognition performance such as the accuracy largely depends on the professional feature engineering and the classifier, which will be more difficult in the scenario of big data. Recently, many emotion researchers trend the direction to automatic emotion recognition from the raw signal, the motivation behind this is that neural network can learn representation and find the final result automatically. This work focuses on categorization and reviews on the current progress on end-to-end speech emotion recognition problems. In this survey, we discuss the requirement of the network model, process procedures and current achiements. We also explore some potential future issues in speech emotion recognition.},
	author = {Zhao, Huijuan and Ye, Ning and Wang, Ruchuan},
	booktitle = {Proceedings - 4th IEEE International Conference on Big Data Security on Cloud, BigDataSecurity 2018, 4th IEEE International Conference on High Performance and Smart Computing, HPSC 2018 and 3rd IEEE International Conference on Intelligent Data and Security, IDS 2018},
	doi = {10.1109/BDS/HPSC/IDS18.2018.00039},
	file = {::},
	isbn = {9781538643990},
	keywords = {big data,end-to-end,neural network,raw data,speech emotion recognition},
	month = {may},
	pages = {139--142},
	publisher = {IEEE},
	title = {{A Survey on Automatic Emotion Recognition Using Audio Big Data and Deep Learning Architectures}},
	url = {https://ieeexplore.ieee.org/document/8552297/},
	year = {2018}
}
@article{Rybka2013,
	abstract = {This paper describes a study of emotion recognition based on speech analysis. The introduction to the theory contains a review of emotion inventories used in various studies of emotion recognition as well as the speech corpora applied, methods of speech parametrization, and the most commonly employed classification algorithms. In the current study the EMO-DB speech corpus and three selected classifiers, the k-Nearest Neighbor (k-NN), the Artificial Neural Network (ANN) and Support Vector Machines (SVMs), were used in experiments. SVMs turned out to provide the best classification accuracy of 75.44{\%} in the speaker dependent mode, that is, when speech samples from the same speaker were included in the training corpus. Various speaker dependent and speaker independent configurations were analyzed and compared. Emotion recognition in speaker dependent conditions usually yielded higher accuracy results than a similar but speaker independent configuration. The improvement was especially well observed if the base recognition ratio of a given speaker was low. Happiness and anger, as well as boredom and neutrality, proved to be the pairs of emotions most often confused.},
	author = {Rybka, Jan and Janicki, Artur},
	doi = {10.2478/amcs-2013-0060},
	file = {::},
	issn = {1641876X},
	journal = {International Journal of Applied Mathematics and Computer Science},
	keywords = {Artificial neural networks,EMO-DB,Emotion recognition,Speech processing,Support vector machines},
	month = {dec},
	number = {4},
	pages = {797--808},
	publisher = {University of Zielona Gora Press},
	title = {{Comparison of speaker dependent and speaker independent emotion recognition}},
	url = {http://content.sciendo.com/view/journals/amcs/23/4/article-p797.xml},
	volume = {23},
	year = {2013}
}
@inproceedings{Kim2018,
	abstract = {In this paper, we propose to use deep 3-dimensional convolutional networks (3D CNNs) in order to address the challenge of modelling spectro-temporal dynamics for speech emotion recognition (SER). Compared to a hybrid of Convolutional Neural Network and Long-Short-Term-Memory (CNN-LSTM), our proposed 3D CNNs simultaneously extract short-term and long-term spectral features with a moderate number of parameters. We evaluated our proposed and other state-of-the-art methods in a speaker-independent manner using aggregated corpora that give a large and diverse set of speakers. We found that 1) shallow temporal and moderately deep spectral kernels of a homogeneous architecture are optimal for the task; and 2) our 3D CNNs are more effective for spectro-temporal feature learning compared to other methods. Finally, we visualised the feature space obtained with our proposed method using t-distributed stochastic neighbour embedding (T-SNE) and could observe distinct clusters of emotions.},
	annote = {Poor},
	author = {Kim, Jaebok and Truong, Khiet P. and Englebienne, Gwenn and Evers, Vanessa},
	booktitle = {2017 7th International Conference on Affective Computing and Intelligent Interaction, ACII 2017},
	doi = {10.1109/ACII.2017.8273628},
	file = {::},
	isbn = {9781538605639},
	month = {oct},
	pages = {383--388},
	publisher = {IEEE},
	title = {{Learning spectro-temporal features with 3D CNNs for speech emotion recognition}},
	url = {http://ieeexplore.ieee.org/document/8273628/},
	volume = {2018-January},
	year = {2018}
}
@inproceedings{Badshah2017,
	abstract = {{\textcopyright} 2017 IEEE. This paper presents a method for speech emotion recognition using spectrograms and deep convolutional neural network (CNN). Spectrograms generated from the speech signals are input to the deep CNN. The proposed model consisting of three convolutional layers and three fully connected layers extract discriminative features from spectrogram images and outputs predictions for the seven emotions. In this study, we trained the proposed model on spectrograms obtained from Berlin emotions dataset. Furthermore, we also investigated the effectiveness of transfer learning for emotions recognition using a pre-trained AlexNet model. Preliminary results indicate that the proposed approach based on freshly trained model is better than the fine-tuned model, and is capable of predicting emotions accurately and efficiently.},
	author = {Badshah, Abdul Malik and Ahmad, Jamil and Rahim, Nasir and Baik, Sung Wook},
	booktitle = {2017 International Conference on Platform Technology and Service, PlatCon 2017 - Proceedings},
	doi = {10.1109/PlatCon.2017.7883728},
	file = {::},
	isbn = {9781509051403},
	keywords = {convolutional neural network,emotions,speech},
	month = {feb},
	pages = {1--5},
	publisher = {IEEE},
	title = {{Speech Emotion Recognition from Spectrograms with Deep Convolutional Neural Network}},
	url = {http://ieeexplore.ieee.org/document/7883728/},
	year = {2017}
}
@article{Nassif2019,
	abstract = {Over the past decades, a tremendous amount of research has been done on the use of machine learning for speech processing applications, especially speech recognition. However, in the past few years, research has focused on utilizing deep learning for speech-related applications. This new area of machine learning has yielded far better results when compared to others in a variety of applications including speech, and thus became a very attractive area of research. This paper provides a thorough examination of the different studies that have been conducted since 2006, when deep learning first arose as a new area of machine learning, for speech applications. A thorough statistical analysis is provided in this review which was conducted by extracting specific information from 174 papers published between the years 2006 and 2018. The results provided in this paper shed light on the trends of research in this area as well as bring focus to new research topics. INDEX},
	author = {Nassif, Ali Bou and Shahin, Ismail and Attili, Imtinan and Azzeh, Mohammad and Shaalan, Khaled},
	doi = {10.1109/ACCESS.2019.2896880},
	file = {::},
	issn = {21693536},
	journal = {IEEE Access},
	keywords = {Speech recognition,deep neural network,systematic review},
	pages = {19143--19165},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	title = {{Speech Recognition Using Deep Neural Networks: A Systematic Review}},
	volume = {7},
	year = {2019}
}
@inproceedings{Burmania2016,
	abstract = {Emotional descriptors collected from perceptual evaluations are important in the study of emotions. Many studies on emotion recognition depend on these labels to train classifiers. The reliability of the emotion descriptors vary with the number and quality of the raters. Conducting perceptual evaluations used to be an expensive and time demanding task, resulting in emotional databases with poor labels annotated by few raters. Nowadays, crowdsourcing services have simplified the process, reducing the cost, facilitating more evaluations per stimuli. The key challenge in using crowdsourcing for perceptual evaluation is the quality which significantly varies across workers. Is it better to have multiple annotations with lower inter-evaluator agreement or to have few annotations with higher inter-evaluator agreement? This study explores this tradeoff between quality and quantity in emotional annotations to characterize expressive behaviors. The analysis relies on emotional labels from the MSP-IMPROV database, where each video was evaluated by over 20 workers. We discuss the theoretical concept of effective reliability to address this problem. We demonstrate that a reduced set of labels with higher inter-evaluator agreement can provide similar classification performance than unfiltered set of labels from multiple workers. We discuss best practices to collecting annotations for emotion recognition tasks using crowdsourcing.},
	author = {Burmania, Alec and Abdelwahab, Mohammed and Busso, Carlos},
	booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
	doi = {10.1109/ICASSP.2016.7472667},
	file = {::},
	isbn = {9781479999880},
	issn = {15206149},
	keywords = {crowdsourcing,emotion recognition,inter-evaluator agreement},
	month = {mar},
	pages = {5190--5194},
	publisher = {IEEE},
	title = {{Tradeoff between quality and quantity of emotional annotations to characterize expressive behaviors}},
	url = {http://ieeexplore.ieee.org/document/7472667/},
	volume = {2016-May},
	year = {2016}
}
@inproceedings{Kim2018a,
	abstract = {To capture variation in categorical emotion recognition by human perceivers, we propose a multi-label learning and evaluation method that can employ the distribution of emotion labels generated by every human annotator. In contrast to the traditional accuracy-based performance measure for categorical emotion labels, our proposed learning and inference algorithms use cross entropy to directly compare human and machine emotion label distributions. Our audiovisual emotion recognition experiments demonstrate that emotion recognition can benefit from using a multi-label representation that fully uses both clear and ambiguous emotion data. Further, the results demonstrate that this emotion recognition system can (i) learn the distribution of human annotators directly; (ii) capture the humanlike label noise in emotion perception; and (iii) identify infrequent or uncommon emotional expression (such as frustration) from inconsistently labeled emotion data, which were often ignored in previous emotion recognition systems.{\textless}br/{\textgreater} {\&}copy; 2018 IEEE.},
	author = {Kim, Yelin and Kim, Jeesun},
	booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
	doi = {10.1109/ICASSP.2018.8462011},
	file = {::},
	isbn = {9781538646588},
	issn = {15206149},
	keywords = {Audio-visual emotion,Emotion recognition,Label noise,Multi -label learning,Prototypicality,Soft labeling},
	month = {apr},
	pages = {5104--5108},
	publisher = {IEEE},
	title = {{Human-Like Emotion Recognition: Multi-Label Learning from Noisy Labeled Audio-Visual Expressive Speech}},
	url = {https://ieeexplore.ieee.org/document/8462011/},
	volume = {2018-April},
	year = {2018}
}
@article{Fayek2017,
	abstract = {Speech Emotion Recognition (SER) can be regarded as a static or dynamic classification problem, which makes SER an excellent test bed for investigating and comparing various deep learning architectures. We describe a frame-based formulation to SER that relies on minimal speech processing and end-to-end deep learning to model intra-utterance dynamics. We use the proposed SER system to empirically explore feed-forward and recurrent neural network architectures and their variants. Experiments conducted illuminate the advantages and limitations of these architectures in paralinguistic speech recognition and emotion recognition in particular. As a result of our exploration, we report state-of-the-art results on the IEMOCAP database for speaker-independent SER and present quantitative and qualitative assessments of the models' performances.},
	author = {Fayek, Haytham M. and Lech, Margaret and Cavedon, Lawrence},
	doi = {10.1016/j.neunet.2017.02.013},
	file = {::},
	issn = {18792782},
	journal = {Neural Networks},
	keywords = {Affective computing,Deep learning,Emotion recognition,Neural networks,Speech recognition},
	month = {aug},
	pages = {60--68},
	publisher = {Pergamon},
	title = {{Evaluating deep learning architectures for Speech Emotion Recognition}},
	url = {https://www.sciencedirect.com/science/article/pii/S089360801730059X?via{\%}3Dihub},
	volume = {92},
	year = {2017}
}
@article{Ekman1992,
	title={An argument for basic emotions},
	volume={6},
	DOI={10.1080/02699939208411068},
	number={3-4},
	journal={Cognition and Emotion},
	author={Ekman, Paul},
	year={1992},
	pages={169-200}
}
@misc{Kozakowski2017,
	title={DCGAN and spectrograms},
	url={http://deepsound.io/dcgan_spectrograms.html},
	journal={DeepSound},
	author={Kozakowski, Piotr and Michalak, Bartosz},
	year={2017}
}
@article{Srivastava2014,
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
	journal = {J. Mach. Learn. Res.},
	issue_date = {January 2014},
	volume = {15},
	number = {1},
	month = jan,
	year = {2014},
	issn = {1532-4435},
	pages = {1929--1958},
	numpages = {30},
	url = {http://dl.acm.org/citation.cfm?id=2627435.2670313},
	acmid = {2670313},
	publisher = {JMLR.org},
	keywords = {deep learning, model combination, neural networks, regularization},
} 
@inproceedings{Ioffe2015,
	author = {Ioffe, Sergey and Szegedy, Christian},
	title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
	booktitle = {Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37},
	series = {ICML'15},
	year = {2015},
	location = {Lille, France},
	pages = {448--456},
	numpages = {9},
	url = {http://dl.acm.org/citation.cfm?id=3045118.3045167},
	acmid = {3045167},
	publisher = {JMLR.org},
}
@article{Cordaro2018,
	title={Universals and cultural variations in 22 emotional expressions across five cultures.},
	volume={18},
	DOI={10.1037/emo0000302},
	number={1},
	journal={Emotion},
	author={Cordaro, Daniel T. and Sun, Rui and Keltner, Dacher and Kamble, Shanmukh and Huddar, Niranjan and McNeil, Galen},
	year={2018},
	pages={75-93}
}
@inproceedings{Burkhardt2005,
	added-at = {2014-12-15T00:00:00.000+0100},
	author = {Burkhardt, Felix and Paeschke, Astrid and Rolfes, M. and Sendlmeier, Walter F. and Weiss, Benjamin},
	biburl = {https://www.bibsonomy.org/bibtex/286f727109180627aadf25dc083ec79b1/dblp},
	booktitle = {INTERSPEECH},
	crossref = {conf/interspeech/2005},
	ee = {http://www.isca-speech.org/archive/interspeech_2005/i05_1517.html},
	interhash = {11b70483ca5c2378516c336ee03b5193},
	intrahash = {86f727109180627aadf25dc083ec79b1},
	keywords = {dblp},
	pages = {1517-1520},
	publisher = {ISCA},
	timestamp = {2015-06-21T06:26:15.000+0200},
	title = {A database of German emotional speech.},
	url = {http://dblp.uni-trier.de/db/conf/interspeech/interspeech2005.html#BurkhardtPRSW05},
	year = 2005
}
@book{Oshaughnessy1990,
	place={Reading, Mass.},
	title={Speech communication},
	publisher={Addison-Wesley Pub. Co.},
	author={O'Shaughnessy, Douglas},
	year={1990}
}



@Comment{jabref-meta: databaseType:biblatex;}
