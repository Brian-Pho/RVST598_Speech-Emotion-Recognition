% Encoding: UTF-8

@article{busso_2008,
	title={IEMOCAP: interactive emotional dyadic motion capture database},
	volume={42},
	DOI={10.1007/s10579-008-9076-6},
	number={4},
	journal={Language Resources and Evaluation},
	author={Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N. and Lee, Sungbok and Narayanan, Shrikanth S.},
	year={2008},
	pages={335-359}
},

@article{cao_2014,
	title={CREMA-D: Crowd-Sourced Emotional Multimodal Actors Dataset},
	volume={5},
	DOI={10.1109/taffc.2014.2336244},
	number={4},
	journal={IEEE Transactions on Affective Computing},
	author={Cao, Houwei and Cooper, David G. and Keutmann, Michael K. and Gur, Ruben C. and Nenkova, Ani and Verma, Ragini},
	year={2014},
	pages={377-390}
},

@article{livingstone_2018,
	title={The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English},
	volume={13},
	DOI={10.1371/journal.pone.0196391},
	number={5},
	journal={PLOS ONE},
	author={Livingstone, Steven R. and Russo, Frank A.},
	year={2018},
	pages={e0196391}
}

@article{dupuis_2011, 
	title={Recognition of emotional speech for younger and older talkers: Behavioural findings from the toronto emotional speech set}, 
	volume={39}, 
	url={https://jcaa.caa-aca.ca/index.php/jcaa/article/view/2471}, 
	abstractNote={A study that was conducted to analyze recognition of emotional speech for younger and older talkers is presented. Each actor recorded the stimuli individually in a sound- attenuating booth for approximately 20 hours. During the recording sessions, which typically lasted three to four hours, the majority of the time was spent creating the voice recordings, while approximately 10% of the time was devoted to practicing and fine-tuning each actorâ€™s portrayal of each of the emotions. Three female undergraduate students with normal hearing listened to the stimuli and identified, for each actor, which token of each NU-6 item they considered to be the most representative for each of the seven emotions. The experimenter used the same procedure to listen to each of the sound files.}, 
	number={3}, 
	journal={Canadian Acoustics}, 
	author={Dupuis, Kate and Kathleen Pichora-Fuller, M.}, 
	year={2011}, 
	month={Sep.}, 
	pages={182-183} 
}

@Comment{jabref-meta: databaseType:biblatex;}
