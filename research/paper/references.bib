% Encoding: UTF-8

@Article{N.Fahlgren,
  author    = {Fahlgren, N. and Gehan, M.A. and Baxter,I.},
  title     = {Lights, camera, action: high-throughput plant phenotyping is ready for a close-up,},
  journal   = {Current Opinion in Plant Biology},
  year      = {2015},
  volume    = {24},
  month     = {apr},
  pages     = {93--99},
  doi       = {10.1016/j.pbi.2015.02.006},
  publisher = {Elsevier {BV}},
}

@Article{L.Li,
  author    = {Li, L. and Zhang, Q. and Huang, D.},
  title     = {A Review of Imaging Techniques for Plant Phenotyping},
  journal   = {Sensors},
  year      = {2014},
  volume    = {14},
  number    = {11},
  month     = {oct},
  pages     = {20078--20111},
  doi       = {10.3390/s141120078},
  publisher = {{MDPI} {AG}},
}

@Article{J.L.Araus,
  author    = {Araus, J.L. and Cairns, E.J.},
  title     = {Field high-throughput phenotyping: the new crop breeding frontier},
  journal   = {Trends in Plant Science},
  year      = {2014},
  volume    = {19},
  number    = {1},
  month     = {jan},
  pages     = {52--61},
  doi       = {10.1016/j.tplants.2013.09.008},
  publisher = {Elsevier {BV}},
}

@InProceedings{D.Tilman,
  author    = {Tilman, David and Balzer, Christian and Hill, Jason and Befort, Belinda L},
  title     = {Global food demand and the sustainable intensification of agriculture},
  year      = {2011},
  volume    = {108},
  number    = {50},
  publisher = {National Acad Sciences},
  pages     = {20260--20264},
  journal   = {Proceedings of the National Academy of Sciences},
}

@Article{D.K.Ray,
  author    = {Ray, D.K. and Ramankutty, N. and Mueller, N.D. and West, P.C. and Foley, J.A.},
  title     = {Recent patterns of crop yield growth and stagnation},
  journal   = {Nature Communications},
  year      = {2012},
  volume    = {3},
  number    = {1},
  month     = {jan},
  doi       = {10.1038/ncomms2296},
  publisher = {Springer Nature},
}

@Article{F.Fiorani,
  author    = {Fabio Fiorani and Ulrich Schurr},
  title     = {Future Scenarios for Plant Phenotyping},
  journal   = {Annual Review of Plant Biology},
  year      = {2013},
  volume    = {64},
  number    = {1},
  month     = {apr},
  pages     = {267--291},
  doi       = {10.1146/annurev-arplant-050312-120137},
  publisher = {Annual Reviews},
}

@Article{P.Canteloube,
  author    = {Canteloube, Pierre and Terres, Jean-Michel},
  title     = {Seasonal weather forecasts for crop yield modelling in Europe},
  journal   = {Tellus A},
  year      = {2005},
  volume    = {57},
  number    = {3},
  month     = {may},
  pages     = {476--487},
  doi       = {10.1111/j.1600-0870.2005.00125.x},
  publisher = {Informa {UK} Limited},
}

@Article{V.R.Reddy,
  author    = {V.R. Reddy and Ya.A. Pachepsky},
  title     = {Predicting crop yields under climate change conditions from monthly GCM weather projections},
  journal   = {Environmental Modelling {\&} Software},
  year      = {2000},
  volume    = {15},
  number    = {1},
  issue     = {1},
  month     = {jan},
  pages     = {79--86},
  doi       = {10.1016/S1364-8152(99)00011-0},
  publisher = {Elsevier {BV}},
}

@Article{R.C.Stone,
  author  = {Stone, Roger C. and Meinke, Holger},
  title   = {Operational seasonal forecasting of crop performance},
  journal = {Philisophical Transactions of The Royal Society B},
  year    = {2005},
  volume  = {360},
  issue   = {1463},
  pages   = {2109--2124},
  doi     = {10.1098/rstb.2005.1753},
}

@Article{A.F.Torres,
  author    = {Torres, Alfonso F and Walker, Wynn R and McKee, Mac},
  title     = {Forecasting daily potential evapotranspiration using machine learning and limited climatic data},
  journal   = {Agricultural Water Management},
  year      = {2011},
  volume    = {98},
  number    = {4},
  pages     = {553--562},
  publisher = {Elsevier},
}

@Article{J.Li,
  author    = {Jiaming Li and John K. Ward and Jingnan Tong and Lyle Collins and Glenn Platt},
  title     = {Machine learning for solar irradiance forecasting of photovoltaic system},
  journal   = {Renewable Energy},
  year      = {2016},
  volume    = {90},
  month     = {may},
  pages     = {542--553},
  doi       = {10.1016/j.renene.2015.12.069},
  publisher = {Elsevier {BV}},
}

@Article{S.Sanz,
  author  = {Sancho Salcedo-Sanz and Emilio G. Ortiz-Garcı and Ángel M. Pérez-Bellido and Antonio Portilla-Figueras and Luis Prieto},
  title   = {Short term wind speed prediction based on evolutionary support vector regression algorithms},
  journal = {Expert Systems with Applications},
  year    = {2011},
  volume  = {38},
  issue   = {4},
  pages   = {4052--4057},
  doi     = {10.1016/j.eswa.2010.09.067},
}

@Article{F.Wang,
  author    = {Wang, Fei and Zhen, Zhao and Mi, Zengqiang and Sun, Hongbin and Su, Shi and Yang, Guang},
  title     = {Solar irradiance feature extraction and support vector machines based weather status pattern recognition model for short-term photovoltaic power forecasting},
  journal   = {Energy and Buildings},
  year      = {2015},
  volume    = {86},
  pages     = {427--438},
  publisher = {Elsevier},
}

@Article{Y.Radhika,
  author    = {Y. Radhika and M. Shashi},
  title     = {Atmospheric Temperature Prediction using Support Vector Machines},
  journal   = {Int. Journal of Computer Theory and Engineering},
  year      = {2009},
  pages     = {55--58},
  doi       = {10.7763/IJCTE.2009.V1.9},
  publisher = {{IACSIT} Press},
}

@Article{A.W.Schaafsma,
  author    = {A.W. Schaafsma and D.C. Hooker},
  title     = {Climatic models to predict occurrence of Fusarium toxins inwheat and maize},
  journal   = {Int. Journal of Food Microbiology},
  year      = {2007},
  volume    = {119},
  issue     = {1},
  pages     = {116--125},
  doi       = {10.1016/j.ijfoodmicro.2007.08.006},
  optannote = {•},
  optkey    = {•},
  optmonth  = {•},
  optnote   = {•},
  optnumber = {•},
  optpages  = {•},
  optvolume = {•},
}
@online{SKLearn,
author = {{\relax scikit-learn developers}},
title = {Documentation scikit-learn: machine learning in Python},
year = {2016},
url = {http://scikit-learn.org/stable/documentation.html},
urldate = {2017-01-30},
note = {Last accesed January 30, 2017},
}

@Article{V.Panwar,
  author    = {Vinay Panwar and Brent McCallum and Guus Bakkeren},
  title     = {Host-induced gene silencing of wheat leaf rust fungus Puccinia triticina pathogenicity genes mediated by the Barley stripe mosaic virus},
  journal   = {Plant Molecular Biology},
  year      = {2013},
  volume    = {81},
  number    = {6},
  month     = {feb},
  pages     = {595--608},
  doi       = {10.1007/s11103-013-0022-7},
  publisher = {Springer Nature},
}

@article{Donahue2018,
	abstract = {While Generative Adversarial Networks (GANs) have seen wide success at the problem of synthesizing realistic images, they have seen little application to audio generation. Unlike for images, a barrier to success is that the best discriminative representations for audio tend to be non-invertible, and thus cannot be used to synthesize listenable outputs. In this paper we introduce WaveGAN, a first attempt at applying GANs to unsupervised synthesis of raw-waveform audio. Our experiments demonstrate that WaveGAN can produce intelligible words from a small vocabulary of speech, and can also synthesize audio from other domains such as drums, bird vocalizations, and piano. Qualitatively, we find that human judges prefer the sound quality of generated examples from WaveGAN over those from a method which na$\backslash$"ively apply GANs on image-like audio feature representations.},
	archivePrefix = {arXiv},
	arxivId = {1802.04208},
	author = {Donahue, Chris and McAuley, Julian and Puckette, Miller},
	doi = {1802.04208},
	eprint = {1802.04208},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/GAN/1802.04208.pdf:pdf},
	issn = {14341948},
	pages = {1--15},
	title = {{Adversarial Audio Synthesis}},
	url = {http://arxiv.org/abs/1802.04208},
	year = {2018}
}

@InCollection{A.Taflove,
  author = {Taflove, Allen and C. Hagness, S},
  title  = {Computational electrodynamics: the finite-difference time-domain method. 2nd ed},
  year   = {2000},
  volume = {67–106},
  isbn   = {1-58053-076-1},
  month  = {06},
}

@article{Brockner2018,
	abstract = {{\textcopyright} 2018 SPIE. The identification followed by avoidance or removal of explosive hazards in past and/or present conflict zones is a serious threat for both civilian and military personnel. This is a challenging task as extreme variability exists with respect to the objects, their environment and emplacement context. A goal is the development of automatic, or human-in-the-loop, sensor technologies that leverage engineering theories like signal processing, data fusion and machine learning. Herein, we explore the detection of buried explosive hazards (BEHs) in handheld ground penetrating radar (HH-GPR) via convolutional neural networks (CNNs). In particular, we investigate the potential for generative adversarial networks (GANs) to impute new data based on limited and class imbalance labeled data. Unsupervised GANs are trained and assessed at a qualitative level and their outputs are explored in different ways to quantitatively help train a CNN classifier. Overall, we found encouraging qualitative results and a list of hurdles that need to be overcome before we anticipate quantitative improvements.},
	author = {Brockner, Blake and Dowdy, Joshua L. and Anderson, Derek T. and Veal, Charlie and Scott, Grant J. and Ball, John E.},
	doi = {10.1117/12.2307261},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/GAN/106280T.pdf:pdf},
	isbn = {9781510617674},
	issn = {1996756X},
	journal = {Detection and Sensing of Mines, Explosive Objects, and Obscured Targets XXIII},
	number = {April},
	pages = {33},
	title = {{Generative adversarial networks for ground penetrating radar in hand held explosive hazard detection}},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10628/2307261/Generative-adversarial-networks-for-ground-penetrating-radar-in-hand-held/10.1117/12.2307261.full},
	year = {2018}
}

@article{Schwegmann2017,
	author = {Schwegmann, C P and Kleynhans, W and Salmon, B P and Mdakane, L W and Meyer, R G V},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/GAN/08127440.pdf:pdf},
	isbn = {9781509049516},
	pages = {2263--2266},
	title = {{Synthetic Aperture Radar Ship Discrimination, Generation, and Latent Variable Extraction using Information Maximizing Generative Adversarial Networks}},
	year = {2017}
}

@article{Guo2017,
	abstract = {Synthetic aperture radar (SAR) image simulators based on computer-aided drawing models play an important role in SAR applications, such as automatic target recognition and image interpretation. However, the accuracy of such simulators is due to geometric error and simplification in the electromagnetic calculation. In this letter, an end-to-end model was developed that could directly synthesize the desired images from the known image database. The model was based on generative adversarial nets (GANs), and its feasibility was validated by comparisons with real images and ray-tracing results. As a further step, the samples were synthesized at angles outside of the data set. However, the training process of GAN models was difficult, especially for SAR images which are usually affected by noise interference. The major failure modes were analyzed in experiments, and a clutter normalization method was proposed to ameliorate them. The results showed that the method improved the speed of convergence up to 10 times. The quality of the synthesized images was also improved.},
	archivePrefix = {arXiv},
	arxivId = {1705.02090},
	author = {Guo, Jiayi and Lei, Bin and Ding, Chibiao and Zhang, Yueting},
	doi = {10.1109/LGRS.2017.2699196},
	eprint = {1705.02090},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/GAN Radar/07927706.pdf:pdf},
	isbn = {1545-598x},
	issn = {1545598X},
	journal = {IEEE Geoscience and Remote Sensing Letters},
	keywords = {Generative adversarial nets (GANs),SAR image simulator,SAR interpretation,synthetic aperture radar (SAR)},
	number = {7},
	pages = {1111--1115},
	publisher = {IEEE},
	title = {{Synthetic Aperture Radar Image Synthesis by Using Generative Adversarial Nets}},
	volume = {14},
	year = {2017}
}

@article{Marmanis2017,
	abstract = {Very High Spatial Resolution (VHSR) large-scale SAR image databases are still an unresolved issue in the Remote Sensing field. In this work, we propose such a dataset and use it to explore patch-based classification in urban and periurban areas, considering 7 distinct semantic classes. In this context, we investigate the accuracy of large CNN classification models and pre-trained networks for SAR imaging systems. Furthermore, we propose a Generative Adversarial Network (GAN) for SAR image generation and test, whether the synthetic data can actually improve classification accuracy.},
	archivePrefix = {arXiv},
	arxivId = {1711.02010},
	author = {Marmanis, Dimitrios and Yao, Wei and Adam, Fathalrahman and Datcu, Mihai and Reinartz, Peter and Schindler, Konrad and Wegner, Jan Dirk and Stilla, Uwe},
	doi = {10.2760/383579},
	eprint = {1711.02010},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/GAN Radar/1711.02010.pdf:pdf},
	isbn = {9789279735271},
	title = {{Artificial Generation of Big Data for Improving Image Classification: A Generative Adversarial Network Approach on SAR Data}},
	url = {http://arxiv.org/abs/1711.02010},
	year = {2017}
}

@inproceedings{Goodfellow2014,
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	archivePrefix = {arXiv},
	arxivId = {1406.2661},
	author = {Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	booktitle = {Neural Information Processing Systems (NIPS)},
	doi = {10.1002/9781118472507.fmatter},
	eprint = {1406.2661},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/GAN Radar/5423-generative-adversarial-nets.pdf:pdf},
	isbn = {9781139058452},
	issn = {10495258},
	month = {June},
	pages = {1-23},
	pmid = {1000183096},
	title = {{Generative Adversarial Networks}},
	url = {http://doi.wiley.com/10.1002/9781118472507.fmatter http://arxiv.org/abs/1406.2661},
	year = {2014}
}


@inproceedings{Anonymous2019,
	abstract = {Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence. Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms. Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modelling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio ∼54,000 times faster than their autoregressive counterparts.},
	author = {Anonymous},
	booktitle = {ICLR OpenReview},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/GAN Radar/GANSynth.pdf:pdf},
	pages = {1-12},
	title = {{Gansynth: Adversarial Neural Audio Synthesis}},
	year = {2019}
}

@inproceedings{Radford2016,
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	archivePrefix = {arXiv},
	arxivId = {1511.06434},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	booktitle = {ICLR},
	doi = {10.1051/0004-6361/201527329},
	eprint = {1511.06434},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/GAN Radar/1511.06434.pdf:pdf},
	isbn = {2004012439},
	issn = {0004-6361},
	month = {May},
	pages = {1-16},
	pmid = {23459267},
	title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
	year = {2016}
}

@book{MATLAB,
	address = {Natick, Massachusetts},
	author = {MATLAB},
	mendeley-groups = {Master's Research},
	publisher = {The MathWorks Inc.},
	title = {{Documentation in version 9.5.0.944444 (R2018b)}},
	year = {2018}
}

@Misc{chollet2015keras,
  author       = {Chollet, Fran\c{c}ois and others},
  title        = {Keras},
  year         = {2015},
  howpublished = {\url{https://github.com/fchollet/keras}},
  publisher    = {GitHub},
}

@article{Reed2016,
	abstract = {Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image model- ing, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.},
	archivePrefix = {arXiv},
	arxivId = {1605.05396},
	author = {Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},
	doi = {10.1017/CBO9781107415324.004},
	eprint = {1605.05396},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/GAN Radar/1605.05396.pdf:pdf},
	isbn = {9781510829008},
	issn = {1550-5499},
	pmid = {25246403},
	title = {{Generative Adversarial Text to Image Synthesis}},
	url = {http://arxiv.org/abs/1605.05396},
	year = {2016}
}

@article{Ko2012,
	abstract = {This paper presents a method to identify landmines in various burial conditions. A ground penetration radar is used to generate data set, which is then processed to reduce the ground effect and noise to obtain landmine signals. Principal components and Fourier coefficients of the landmine signals are computed, which are used as features of each landmine for detection and identification. A database is constructed based on the features of various types of landmines and the ground conditions, including the different levels of moisture and types of ground and the burial depths of the landmines. Detection and identification is performed by searching for features in the database. For a robust decision, the counting method and the Mahalanobis distance-based likelihood ratio test method are employed. Four landmines, different in size and material, are considered as examples that demonstrate the efficiency of the proposed method for detecting and identifying landmines. {\textcopyright} 2012 Kwang Hee Ko et al.},
	author = {Ko, Kwang Hee and Jang, Gyubin and Park, Kyungmi and Kim, Kangwook},
	doi = {10.1155/2012/826404},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/GAN Radar/826404.pdf:pdf},
	issn = {16875869},
	journal = {Int. Journal of Antennas and Propagation},
	title = {{GPR-based landmine detection and identification using multiple features}},
	volume = {2012},
	year = {2012}
}

@article{Pitcher2018,
	author = {Pitcher, Aaron D. and McCombe, Justin J. and Eveleigh, Eric A. and Nikolova, Natalia K.},
	doi = {10.1109/MWSYM.2018.8439660},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/GAN Radar/08439660.pdf:pdf},
	isbn = {9781538650677},
	issn = {0149645X},
	journal = {IEEE MTT-S Int. Microwave Symposium Digest},
	keywords = {Concealed weapon detection,Jitter,Noise,Pulsed radar,Radar,Ultra-wideband},
	pages = {919--922},
	publisher = {IEEE},
	title = {{Compact Transmitter for Pulsed-Radar Detection of On-Body Concealed Weapons}},
	volume = {2018-June},
	year = {2018}
}

@article{Truong2018,
	abstract = {{\textless}p{\textgreater}Understanding the root system architecture of plants as they develop is critical for increasing crop yields through plant phenotyping, and ultra-wideband imaging systems have shown potential as a portable, low-cost solution to non-destructive imaging root system architectures. This paper presents the design, implementation, and analysis of an ultra-wideband imaging system for use in imaging potted plant root system architectures. The proposed system is separated into three main subsystems: a Data Acquisition module, a Data Processing module, and an Image Processing and Analysis module. The Data Acquisition module consists of simulated and experimental implementations of a non-contact synthetic aperture radar system to measure ultra-wideband signal reflections from concealed scattering objects in a pot containing soil. The Data Processing module is responsible for interpreting the measured ultra-wideband signals and producing an image using a delay-and-sum beamforming algorithm. The Image Processing and Analysis module is responsible for improving image quality and measuring root depth and average root diameter in an unsupervised manner. The Image Processing and Analysis module uses a modified top-hat transformation alongside quantization methods based on energy distributions in the image to isolate the surface of the imaged root. Altogether, the proposed subsystems are capable of imaging and measuring concealed taproot system architectures with controlled soil conditions; however, the performance of the system is highly dependent on knowledge of the soil conditions. Smaller roots in difficult imaging conditions require future work into understanding and compensating for unwanted noise. Ultimately, this paper sought to provide insight into improving imaging quality of ultra-wideband (UWB) imaging systems for plant root imaging for other works to be followed.{\textless}/p{\textgreater}},
	author = {Truong, Thomas and Dinh, Anh and Wahid, Khan},
	doi = {10.3390/s18082438},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/GAN Radar/sensors-18-02438.pdf:pdf},
	issn = {14248220},
	journal = {Sensors (Switzerland)},
	keywords = {Delay-and-sum beamforming,Image processing,Non-destructive root imaging,Radar imaging,Ultra-wideband},
	number = {8},
	pmid = {30050024},
	title = {{An ultra-wideband frequency system for non-destructive root imaging}},
	volume = {18},
	year = {2018}
}

@article{Preece2016,
	abstract = {Abstract. A microwave imaging system has been developed as a clinical diagnostic tool operating in the 3- to 8-GHz region using multistatic data collection. A total of 86 patients recruited from a symptomatic breast care clinic were scanned with a prototype design. The resultant three-dimensional images have been compared “blind” with available ultrasound and mammogram images to determine the detection rate. Images show the location of the strongest signal, and this corresponded in both older and younger women, with sensitivity of {\textgreater}74{\%}, which was found to be maintained in dense breasts. The pathway from clinical prototype to clinical evalu- ation is outlined.},
	author = {Preece, Alan W. and Craddock, Ian and Shere, Mike and Jones, Lyn and Winton, Helen L.},
	doi = {10.1021/ic50143a052},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/GAN Radar/JMI{\_}3{\_}3{\_}033502.pdf:pdf},
	issn = {1520510X},
	journal = {Journal of Medical Imaging},
	keywords = {20,2016,3,30,accepted for publication jun,breast cancer,clinical evaluation,imaging,maria,multistatic radar,paper 16024r received feb,published online jul,ultrawideband},
	number = {3},
	pages = {033502},
	title = {{MARIA M4: clinical evaluation of a prototype ultrawideband radar scanner for breast cancer detection}},
	volume = {3},
	year = {2016}
}

@article{Abouelenien2017,
	archivePrefix = {arXiv},
	arxivId = {cond-mat/9305036},
	author = {Abouelenien, Mohamed and Perez-Rosas, Veronica and Mihalcea, Rada and Burzo, Mihai},
	doi = {10.1109/TIFS.2016.2639344},
	eprint = {9305036},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/GAN Radar/07782429.pdf:pdf},
	issn = {15566013},
	journal = {IEEE Transactions on Information Forensics and Security},
	keywords = {Deception detection,linguistic features,multimodal processing,physiological features,thermal features},
	number = {5},
	pages = {1042--1055},
	primaryClass = {cond-mat},
	title = {{Detecting Deceptive Behavior via Integration of Discriminative Features from Multiple Modalities}},
	volume = {12},
	year = {2017}
}


@article{Antoniou2017,
	abstract = {Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data. Given there is potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, takes data from a source domain and learns to take any data item and generalise it to generate other within-class data items. As this generative process does not depend on the classes themselves, it can be applied to novel unseen classes of data. We show that a Data Augmentation Generative Adversarial Network (DAGAN) augments standard vanilla classifiers well. We also show a DAGAN can enhance few-shot learning systems such as Matching Networks. We demonstrate these approaches on Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In our experiments we can see over 13{\%} increase in accuracy in the low-data regime experiments in Omniglot (from 69{\%} to 82{\%}), EMNIST (73.9{\%} to 76{\%}) and VGG-Face (4.5{\%} to 12{\%}); in Matching Networks for Omniglot we observe an increase of 0.5{\%} (from 96.9{\%} to 97.4{\%}) and an increase of 1.8{\%} in EMNIST (from 59.5{\%} to 61.3{\%}).},
	archivePrefix = {arXiv},
	arxivId = {1711.04340},
	author = {Antoniou, Antreas and Storkey, Amos and Edwards, Harrison},
	doi = {10.1088/1751-8113/44/44/445203},
	eprint = {1711.04340},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/GAN Radar/1711.04340.pdf:pdf},
	isbn = {9781538637876},
	issn = {17518113},
	pages = {1--14},
	pmid = {16054681},
	title = {{Data Augmentation Generative Adversarial Networks}},
	url = {http://arxiv.org/abs/1711.04340},
	year = {2017}
}

@phdthesis{Hutchinson2015,
	author = {Hutchinson, Simon James},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/UWB-model/Formalization/!!!Conceal-Object-Thesis-Hutchinson-2015.pdf:pdf},
	mendeley-groups = {UWB Weapon Detection},
	school = {Manchester Metropolitan University},
	title = {{Investigation of Late Time Response Analysis for Security Applications}},
	year = {2015}
}

@article{Lee2017,
	author = {Lee, Seung-Jae and Choi, In-Sik and Chae, Dae-Young},
	doi = {10.1080/09205071.2017.1324324},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/UWB-model/Formalization/!!!Conceal-Object-LTR-Lee-2017.pdf:pdf},
	issn = {0920-5071},
	journal = {Journal of Electromagnetic Waves and Applications},
	keywords = {Radar target classification,complex natural resonance frequency,feature extraction,feature vector fusion,waveform structure-based feature},
	mendeley-groups = {UWB Weapon Detection},
	number = {10},
	pages = {1020--1033},
	publisher = {Taylor {\&} Francis},
	title = {{A novel feature extraction method for radar target classification using fusion of early-time and late-time regions}},
	volume = {31},
	year = {2017}
}

@article{Selver2016,
	abstract = {{\textcopyright} 2016 IEEE.Classification of objects from scattered electromagnetic waves is a difficult problem, as it heavily depends on aspect angle. To minimize this dependency, distinguishable features can be used. In this paper, we propose a target identification method in the resonance scattering region using a novel structural feature set based on the scattered signal waveform. To obtain robustness at low signal-to-noise ratio (SNR), a multiscale approximation is used for distortion correction prior to the feature extraction. This is achieved by an overlapping grid hierarchical radial basis function (HRBFOG) network topology, which is demonstrated to outperform existing HRBF techniques. The results obtained from the simulations and the measurements performed for various targets show high accuracy for classification with the proposed feature set, robustness through the use of HRBF at low SNR, and efficient computation in real time.},
	author = {Selver, Mustafa Alper and Taygur, Mehmet Mert and Se??men, Mustafa and Zoral, Emine Ye??im},
	doi = {10.1109/TAP.2016.2567438},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/UWB-model/Formalization/!!!Target-Classification-Selver-2016.pdf:pdf},
	issn = {0018926X},
	journal = {IEEE Transactions on Antennas and Propagation},
	keywords = {Multiscale analysis (MSA),neural networks (NNs),resonance scattering region,target classification,time domain analysis},
	mendeley-groups = {UWB Weapon Detection},
	number = {7},
	pages = {3120--3129},
	title = {{Hierarchical Reconstruction and Structural Waveform Analysis for Target Classification}},
	volume = {64},
	year = {2016}
}

@article{Xia2013,
	abstract = {Ultra-wideband (UWB) technology has been widely utilized in radar system because of the advantage of the ability of high spatial resolution and object-distinction capability. A major challenge in UWB signal processing is the requirement for very high sampling rate under Shannon-Nyquist sampling theorem which exceeds the current ADC capacity. Recently, new approaches based on the Finite Rate of Innovation (FRI) allow significant reduction in the sampling rate. A system for sampling UWB radar echo signal at an ultra-low sampling rate and the estimation of time-delays is presented in the paper. An ultra-low rate sampling scheme based on FRI is applied, which often results in sparse parameter extraction for UWB radar signal detection. The parameters such as time-delays are estimated using the framework of compressed sensing based on total-variation norm minimization. With this system, the UWB radar signal can be accurately reconstructed and detected with overwhelming probability at the rate much lower than Nyquist rate. The simulation results show that the proposed method is effective for sampling and detecting UWB radar signal at an ultra-low sampling rate. 2013 SPIE.},
	author = {Xia, Shugao and Sichina, Jeffrey and Liu, Fengshan},
	doi = {10.1117/12.2015867},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/UWB-model/Formalization/Feature-UWB-Radar-Li-2013.pdf:pdf},
	isbn = {0277786X},
	journal = {Compressive Sensing II, May 2, 2013 - May 3, 2013},
	keywords = {Compressed sensing,Parameter extraction,Radar systems,Signal detection,Signal reconstruction,Signal sampling,Synthetic aperture radar,Time delay,Ultra-wideband (UWB)},
	mendeley-groups = {UWB Weapon Detection},
	number = {May 2013},
	pages = {87170M},
	title = {{UWB radar echo signal detection based on compressive sensing}},
	volume = {8717},
	year = {2013}
}

@article{Seyfioglu2018,
	abstract = {Radar-based activity recognition is a problem that has been of great interest due to applications such as border control and security, pedestrian identification for automotive safety, and remote health monitoring. This work seeks to show the efficacy of micro-Doppler analysis to distinguish even those gaits whose micro-Doppler signatures are not visually distinguishable. Moreover, a 3-layer, deep convolutional autoencoder (CAE) is proposed, which utilizes unsupervised pre-training to initialize the weights in the subsequent convolutional layers. This architecture is shown to be more effective than other deep learning architectures, such as convolutional neural networks (CNN) and autoencoders (AE), as well as conventional classifiers employing pre-defined features, such as support vector machines (SVM), random forest (RF) and extreme gradient boosting (Xgboost). Results show the performance of the proposed deep CAE yields a correct classification rate of 94.2{\%} for micro-Doppler signatures of 12 different human activities measured indoors using a 4 GHz continuous wave radar - 17.3{\%} improvement over SVM. IEEE},
	author = {Seyfioǧlu, Mehmet Saygin and {\"{O}}zbayoǧlu, Ahmet Murat and G{\"{u}}rb{\"{u}}z, Sevgi Zubeyde},
	doi = {10.1109/TAES.2018.2799758},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/GAN Radar/08283539.pdf:pdf},
	isbn = {00189251 (ISSN)},
	issn = {00189251},
	journal = {IEEE Transactions on Aerospace and Electronic Systems},
	keywords = {Convolutional autoencoder (CAE),deep learning,gait recognition,micro-Doppler,neural networks,radar},
	number = {4},
	pages = {1709--1723},
	title = {{Deep convolutional autoencoder for radar-based classification of similar aided and unaided human activities}},
	volume = {54},
	year = {2018}
}

@InProceedings{Gang2017,
	author="Zheng, Gang
	and Ji, Shengzhen
	and Dai, Min
	and Sun, Ying",
	editor="Zhou, Jie
	and Wang, Yunhong
	and Sun, Zhenan
	and Xu, Yong
	and Shen, Linlin
	and Feng, Jianjiang
	and Shan, Shiguang
	and Qiao, Yu
	and Guo, Zhenhua
	and Yu, Shiqi",
	title="ECG Based Identification by Deep Learning",
	booktitle="Biometric Recognition",
	year="2017",
	publisher="Springer Int. Publishing",
	address="Cham",
	pages="503--510",
	abstract="Strategies were proposed for Electrocardiogram (ECG) based identification. Firstly, a selecting mechanism based on information entropy was used to obtain whole heart beat signal; Secondly, a Depth Neural Network (DNN) based on Denoising AutoEncoder (DAE) was adopted in feature selection unsupervised, by which, the robustness of the recognition system could be improved in recognizing. Finally, 98.10{\%} and 95.67{\%} recognition rate were obtained on self-collected calm and high pressure data sets respectively, and 94.39{\%} rate on combined data sets of MIT arrhythmia database (mitdb) and self-collected data averagely.",
	isbn="978-3-319-69923-3"
}

@article{Pathoumvanh2013,
	abstract = {Electrocardiogram (ECG) has been actively proposed as aliveness biometric. In this paper, the study which concern to a realistic application is proposed. Firstly, a single lead normal ECG signal is acquired from individuals of 10 subjects. Then, each single beat ECG is segmented and analyzed in Continuous Wavelet Transform (CWT) domain. Total energy of wavelet coefficients for each P, QRS, and T segment is calculated. Next, the Fisher Linear Discriminant Analysis (FLDA) is applied. Finally, normalized Euclidean distance is implemented as a classifier. In experimental results, 97{\%} of classification accuracy is achieved in case of a normal ECG (with non-variation of heart rate).},
	author = {Pathoumvanh, Somsanouk and Airphaiboon, Surapan and Prapochanung, Benjawan and Leauhatong, Thurdsak},
	doi = {10.1109/BMEiCon.2013.6687703},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/ECG Identification/06687703.pdf:pdf},
	isbn = {9781479914678},
	journal = {BMEiCON 2013 - 6th Biomedical Engineering Int. Conference},
	keywords = {ECG Biometrics,ECG Identifications,Single Beat ECG features extraction},
	number = {7},
	pages = {1--4},
	publisher = {IEEE},
	title = {{ECG analysis for person identification}},
	year = {2013}
}

@article{Fratini2015,
	abstract = {BACKGROUND: During last decade the use of ECG recordings in biometric recognition studies has increased. ECG characteristics made it suitable for subject identification: it is unique, present in all living individuals, and hard to forge. However, in spite of the great number of approaches found in literature, no agreement exists on the most appropriate methodology. This study aimed at providing a survey of the techniques used so far in ECG-based human identification. Specifically, a pattern recognition perspective is here proposed providing a unifying framework to appreciate previous studies and, hopefully, guide future research. METHODS: We searched for papers on the subject from the earliest available date using relevant electronic databases (Medline, IEEEXplore, Scopus, and Web of Knowledge). The following terms were used in different combinations: electrocardiogram, ECG, human identification, biometric, authentication and individual variability. The electronic sources were last searched on 1st March 2015. In our selection we included published research on peer-reviewed journals, books chapters and conferences proceedings. The search was performed for English language documents. RESULTS: 100 pertinent papers were found. Number of subjects involved in the journal studies ranges from 10 to 502, age from 16 to 86, male and female subjects are generally present. Number of analysed leads varies as well as the recording conditions. Identification performance differs widely as well as verification rate. Many studies refer to publicly available databases (Physionet ECG databases repository) while others rely on proprietary recordings making difficult them to compare. As a measure of overall accuracy we computed a weighted average of the identification rate and equal error rate in authentication scenarios. Identification rate resulted equal to 94.95 {\%} while the equal error rate equal to 0.92 {\%}. CONCLUSIONS: Biometric recognition is a mature field of research. Nevertheless, the use of physiological signals features, such as the ECG traits, needs further improvements. ECG features have the potential to be used in daily activities such as access control and patient handling as well as in wearable electronics applications. However, some barriers still limit its growth. Further analysis should be addressed on the use of single lead recordings and the study of features which are not dependent on the recording sites (e.g. fingers, hand palms). Moreover, it is expected that new techniques will be developed using fiducials and non-fiducial based features in order to catch the best of both approaches. ECG recognition in pathological subjects is also worth of additional investigations.},
	author = {Fratini, Antonio and Sansone, Mario and Bifulco, Paolo and Cesarelli, Mario},
	doi = {10.1186/s12938-015-0072-y},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/ECG Identification/12938{\_}2015{\_}Article{\_}72.pdf:pdf},
	issn = {1475925X},
	journal = {BioMedical Engineering Online},
	number = {1},
	pages = {1--23},
	publisher = {BioMed Central},
	title = {{Individual identification via electrocardiogram analysis}},
	volume = {14},
	year = {2015}
}

@article{Salloum2017,
	author = {Salloum, Ronald and {Jay Kuo}, C.-C},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/ECG Identification/07952519.pdf:pdf},
	isbn = {9781509041176},
	journal = {IEEE Int. Conference on Acoustics, Speech, and Signal Processing 2017},
	pages = {2062--2066},
	title = {{ECG-Based Biometrics Using Recurrent Neural Network}},
	year = {2017}
}

@article{Hezil2017,
	abstract = {{\textcopyright} The Institution of Engineering and Technology 2017. Combining multiple human trait features is a proven and effective strategy for biometric-based personal identification. In this study, the authors investigate the fusion of two biometric modalities, i.e. ear and palmprint, at feature-level. Ear and palmprint patterns are characterised by a rich and stable structure, which provides a large amount of information to discriminate individuals. Local texture descriptors, namely local binary patterns, weber local descriptor, and binarised statistical image features, were used to extract the discriminant features for robust human identification. The authors' extensive experimental analysis based on the benchmark IIT Delhi-2 ear and IIT Delhi palmprint databases confirmed that the proposed multimodal biometric system is able to increase recognition rates compared with that produced by single-modal biometrics, attaining a recognition rate of 100{\%}.},
	author = {Hezil, Nabil and Boukrouche, Abdelhani},
	doi = {10.1049/iet-bmt.2016.0072},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/ECG Identification/08016836.pdf:pdf},
	issn = {2047-4938},
	journal = {IET Biometrics},
	number = {5},
	pages = {351--359},
	title = {{Multimodal biometric recognition using human ear and palmprint}},
	volume = {6},
	year = {2017}
}

@article{Sarhan2017,
	abstract = {Biometrics technology stands as one of the major backbones that had united biosciences and technology representing an instrument for security and forensics researchers to develop more accurate, robust and confident systems. Starting from uni-modal biometrics as finger print, face, speech and iris passing through multimodal biometrics based on uni-biometrics fused by different fusion techniques as feature level, score level and decision level fusion techniques, biometrics were still one of the most investigated technologies. From here in this paper, we tried to build the base for researchers whom are interested in biometric systems through introducing a comparative study of most used and known uni- and multimodal biometrics such as face, iris, finger vein, face and iris multimodal, face, finger print and finger vein multimodal. Through this comparative study, a comparative model is based on principal component analysis feature extractor and Euclidean distance matcher applied using MATLAB. This model was trained and tested in two different modes homogenous data using SDUMLA-HMT database and heterogeneous mode extracting 106 frontal single face image from CASIA-FACEV5 while the reminder biometrics under consideration from SDUMLA-HMT. Feature level and score level fusions were tested in both modes on all multimodal systems under consideration.},
	author = {Sarhan, Shahenda and Alhassan, Shaaban and Elmougy, Samir},
	doi = {10.1007/s13369-016-2241-0},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/ECG Identification/Sarhan2017{\_}Article{\_}MultimodalBiometricSystemsACom.pdf:pdf},
	issn = {21914281},
	journal = {Arabian Journal for Science and Engineering},
	keywords = {Feature level fusion,Heterogeneous data,Homogenous data,Multimodal biometrics,Score level fusion,Uni-modal biometrics},
	number = {2},
	pages = {443--457},
	title = {{Multimodal Biometric Systems: A Comparative Study}},
	volume = {42},
	year = {2017}
}

@Article{faundez2005data,
  author    = {Faundez-Zanuy, Marcos},
  title     = {Data fusion in biometrics},
  journal   = {IEEE Aerospace and Electronic Systems Magazine},
  year      = {2005},
  volume    = {20},
  number    = {1},
  pages     = {34--38},
  publisher = {IEEE},
}

@Article{wilson2006facial,
  author    = {Wilson, Phillip Ian and Fernandez, John},
  title     = {Facial feature detection using Haar classifiers},
  journal   = {Journal of Computing Sciences in Colleges},
  year      = {2006},
  volume    = {21},
  number    = {4},
  pages     = {127--133},
  publisher = {Consortium for Computing Sciences in Colleges},
}

@Book{bradski2008learning,
  author    = {Bradski, Gary and Kaehler, Adrian},
  title     = {Learning OpenCV: Computer vision with the OpenCV library},
  year      = {2008},
  publisher = {" O'Reilly Media, Inc."},
  isbn      = {978--0--596--51613--0},
}

@Book{varshney2012distributed,
  author    = {Varshney, Pramod K},
  title     = {Distributed detection and data fusion},
  year      = {2012},
  publisher = {Springer Science \& Business Media},
}

@InProceedings{Szegedy_2016_CVPR,
  author        = {Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jonathon Shlens and Zbigniew Wojna},
  title         = {Rethinking the Inception Architecture for Computer Vision},
  year          = {2015},
  volume        = {abs/1512.00567},
  eprint        = {1512.00567},
  url           = {http://arxiv.org/abs/1512.00567},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/SzegedyVISW15},
  journal       = {CoRR},
  timestamp     = {Mon, 13 Aug 2018 16:49:07 +0200},
}

@Misc{TensorflowInecption,
  author = {{Google Inc.}},
  title  = {Inception in TensorFlow},
  year   = {2016},
  note   = {[Online; accessed April 9, 2019]},
  url    = {https://raw.githubusercontent.com/tensorflow/models/master/research/inception/g3doc/inception_v3_architecture.png},
}

@InProceedings{werner2013towards,
  author    = {Werner, Philipp and Al-Hamadi, Ayoub and Niese, Robert and Walter, Steffen and Gruss, Sascha and Traue, Harald C},
  title     = {Towards pain monitoring: Facial expression, head pose, a new database, an automatic system and remaining challenges},
  booktitle = {Proceedings of the British Machine Vision Conference},
  year      = {2013},
  pages     = {1--13},
}

@InProceedings{walter2013biovid,
  author       = {Walter, Steffen and Gruss, Sascha and Ehleiter, Hagen and Tan, Junwen and Traue, Harald C and Werner, Philipp and Al-Hamadi, Ayoub and Crawcour, Stephen and Andrade, Adriano O and da Silva, Gustavo Moreira},
  title        = {The biovid heat pain database data for the advancement and systematic validation of an automated pain recognition system},
  booktitle    = {2013 IEEE int. conference on cybernetics (CYBCO)},
  year         = {2013},
  organization = {IEEE},
  pages        = {128--131},
}

@Article{sajjad2018cnn,
  author    = {Muhammad Sajjad and Salman Khan and Tanveer Hussain and Khan Muhammad and Arun Kumar Sangaiah and Aniello Castiglione and Christian Esposito and Sung Wook Baik},
  title     = {CNN-based anti-spoofing two-tier multi-factor authentication system},
  journal   = {Pattern Recognition Letters},
  year      = {2018},
  month     = {Feb.},
  doi       = {10.1016/j.patrec.2018.02.015},
  publisher = {Elsevier},
}

@Article{sajjad2017raspberry,
  author    = {Muhammad Sajjad and Mansoor Nasir and Khan Muhammad and Siraj Khan and Zahoor Jan and Arun Kumar Sangaiah and Mohamed Elhoseny and Sung Wook Baik},
  title     = {Raspberry Pi assisted face recognition framework for enhanced law-enforcement services in smart cities},
  journal   = {Future Generation Computer Systems},
  year      = {2017},
  month     = {Nov.},
  doi       = {10.1016/j.future.2017.11.013},
  publisher = {Elsevier},
}

@Article{wang2008robust,
  author  = {Wang, Fenghua and Han, Jiuqiang},
  title   = {Robust multimodal biometric authentication integrating iris, face and palmprint},
  journal = {Information technology and control},
  year    = {2008},
  volume  = {37},
  number  = {4},
}

@Article{maurer2008fusing,
  author    = {Maurer, Donald E and Baker, John P},
  title     = {Fusing multimodal biometrics with quality estimates via a Bayesian belief network},
  journal   = {Pattern Recognition},
  year      = {2008},
  volume    = {41},
  number    = {3},
  pages     = {821--832},
  publisher = {Elsevier},
}

@InProceedings{samaria1994parameterisation,
  author       = {Samaria, Ferdinando S and Harter, Andy C},
  title        = {Parameterisation of a stochastic model for human face identification},
  booktitle    = {Proceedings of 1994 IEEE Workshop on Applications of Computer Vision},
  year         = {1994},
  organization = {IEEE},
  pages        = {138--142},
}

@Article{pan2010survey,
  author    = {Pan, Sinno Jialin and Yang, Qiang},
  title     = {A survey on transfer learning},
  journal   = {IEEE Trans. on knowledge and data engineering},
  year      = {2010},
  volume    = {22},
  number    = {10},
  pages     = {1345--1359},
  publisher = {IEEE},
}

@Article{simonyan2014very,
  author  = {Simonyan, Karen and Zisserman, Andrew},
  title   = {Very deep convolutional networks for large-scale image recognition},
  journal = {arXiv preprint arXiv:1409.1556},
  year    = {2014},
}


@article{Jain2005,
	abstract = {Multimodal biometric systems consolidate the evidence presented by multiple biometric sources and typically provide better recognition performance compared to systems based on a single biometric modality. Although information fusion in a multimodal system can be performed at various levels, integration at the matching score level is the most common approach due to the ease in accessing and combining the scores generated by different matchers. Since the matching scores output by the various modalities are heterogeneous, score normalization is needed to transform these scores into a common domain, prior to combining them. In this paper, we have studied the performance of different normalization techniques and fusion rules in the context of a multimodal biometric system based on the face, fingerprint and hand-geometry traits of a user. Experiments conducted on a database of 100 users indicate that the application of min-max, z-score, and tanh normalization schemes followed by a simple sum of scores fusion method results in better recognition performance compared to other methods. However, experiments also reveal that the min-max and z-score normalization techniques are sensitive to outliers in the data, highlighting the need for a robust and efficient normalization procedure like the tanh normalization. It was also observed that multimodal systems utilizing user-specific weights perform better compared to systems that assign the same set of weights to the multiple biometric traits of all users. {\textcopyright} 2005 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.},
	author = {Jain, Anil and Nandakumar, Karthik and Ross, Arun},
	doi = {10.1016/j.patcog.2005.01.012},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/ECG Identification/1-s2.0-S0031320305000592-main.pdf:pdf},
	issn = {00313203},
	journal = {Pattern Recognition},
	keywords = {Biometrics,Face,Fingerprint,Hand-geometry,Min-max,Multibiometrics,Parzen,Score normalization,Sigmoid,Tanh,User-specific weights,z-score},
	number = {12},
	pages = {2270--2285},
	title = {{Score normalization in multimodal biometric systems}},
	volume = {38},
	year = {2005}
}

@article{Israel2004,
	abstract = { Single modality biometric identification systems exhibit performance that may not be adequate for many security applications. Face and fingerprint modalities dominate the biometric verification/identification field. However, both face and fingerprint can be compromised using counterfeit credentials. Previous research has demonstrated the use of the electrocardiogram (ECG) as a novel biometric. This paper explores the fusion of a traditional face recognition technique with ECG. System performance with multimodality fusion can be superior to reliance on a single biometric, but performance depends heavily on the fusion technique. In addition, a fusion-based system is more difficult to defeat, since an imposter must provide counterfeit credentials for both face and cardiovascular function.},
	author = {Israel, S. A. and Scruggs, W. T. and Worek, W. J. and Irvine, J. M.},
	doi = {10.1109/AIPR.2003.1284276},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/ECG Identification/01284276.pdf:pdf},
	isbn = {0769520294},
	issn = {21642516},
	journal = {Proceedings - Applied Imagery Pattern Recognition Workshop},
	pages = {226--231},
	title = {{Fusing face and ECG for personal identification}},
	volume = {2003-January},
	year = {2004}
}

@article{Boumbarov2012,
	author = {Boumbarov, Ognian and Velchev, Yuliyan and Tonchev, Krasimir and Paliy, Igor},
	doi = {10.5772/21842},
	file = {:C$\backslash$:/Users/Thomas/OneDrive - University of Calgary/PhD/Research Papers/ECG Identification/10.1.1.456.5651.pdf:pdf},
	journal = {Advanced Biometric Technologies},
	title = {{Face and ECG Based Multi-Modal Biometric Authentication}},
	year = {2012}
}

@Comment{jabref-meta: databaseType:biblatex;}
